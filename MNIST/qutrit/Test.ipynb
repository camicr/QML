{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e55e1-5032-4a00-8e83-1f32795d9cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## General SU(N)-----------------\n",
    "class QNN_0(nn.Module):\n",
    "    def __init__(self, num_layers, num_features, num_params, init_weights=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.num_features = num_features\n",
    "        self.num_params = num_params\n",
    "\n",
    "        # Qutrit basis\n",
    "        q0 = torch.tensor([[1], [0], [0]], dtype=torch.complex64)\n",
    "        q1 = torch.tensor([[0], [1], [0]], dtype=torch.complex64)\n",
    "        q2 = torch.tensor([[0], [0], [1]], dtype=torch.complex64)\n",
    "        self.register_buffer(\"q0\", q0)\n",
    "        self.register_buffer(\"q1\", q1)\n",
    "        self.register_buffer(\"q2\", q2)\n",
    "\n",
    "        # Outer product helper\n",
    "        gm = lambda A, B: torch.kron(A, B.T)\n",
    "        \n",
    "        # Gell-Mann generators\n",
    "        gm1 = (gm(q0, q1) + gm(q1, q0)).to(torch.complex64)\n",
    "        gm2 = (-1j * (gm(q0, q1) - gm(q1, q0))).to(torch.complex64)\n",
    "        gm3 = (gm(q0, q0) - gm(q1, q1)).to(torch.complex64)\n",
    "        gm4 = (gm(q0, q2) + gm(q2, q0)).to(torch.complex64)\n",
    "        gm5 = (-1j * (gm(q0, q2) - gm(q2, q0))).to(torch.complex64)\n",
    "        gm6 = (gm(q1, q2) + gm(q2, q1)).to(torch.complex64)\n",
    "        gm7 = (-1j * (gm(q1, q2) - gm(q2, q1))).to(torch.complex64)\n",
    "        gm8 = (1 / torch.sqrt(torch.tensor(3., dtype=torch.float32)) * (gm(q0, q0) + gm(q1, q1) - 2 * gm(q2, q2))).to(torch.complex64)\n",
    "                \n",
    "        # Gell-Mann generators\n",
    "        # generators = [\n",
    "        #             gm3,\n",
    "        #             gm2,\n",
    "        #             gm3,\n",
    "        #             gm5,\n",
    "        #             gm3,\n",
    "        #             gm2,\n",
    "        #             gm3,\n",
    "        #             gm8]\n",
    "\n",
    "        generators = [\n",
    "                    gm8,\n",
    "                    gm3,\n",
    "                    gm2,\n",
    "                    gm3,\n",
    "                    gm5,\n",
    "                    gm3,\n",
    "                    gm2,\n",
    "                    gm3]\n",
    "     \n",
    "        # Subset para codificación, todos para la parte variacional\n",
    "        self.register_buffer(\"gens_enc\", torch.stack(generators[:self.num_features]))  # [F, 3, 3]\n",
    "        self.register_buffer(\"gens_var\", torch.stack(generators[:self.num_params]))    # [P, 3, 3]\n",
    "        \n",
    "        # Label projectors: |0><0|, |1><1|, |2><2|\n",
    "        self.register_buffer(\"label_ops\", torch.stack([gm(q, q) for q in [q0, q1, q2]]))\n",
    "\n",
    "        # initial state\n",
    "        self.q0 = q0\n",
    "\n",
    "        # parameters per layer\n",
    "        self.weights = nn.ParameterList([\n",
    "            nn.Parameter(init_weights[i] if init_weights else torch.rand(6)*2 - 1)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch_size = batch.shape[0]\n",
    "        batch_c = batch.to(torch.cfloat)\n",
    "\n",
    "        state = self.q0.expand(batch_size, -1, -1).clone()\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # Encoding\n",
    "            for j in range(self.num_features):\n",
    "                G = self.gens_enc[j]\n",
    "                x = batch_c[:, j].view(-1, 1, 1)\n",
    "                U = torch.matrix_exp(1j * x * G)\n",
    "                state = torch.bmm(U, state)\n",
    "\n",
    "            # Variational\n",
    "            for j, G in enumerate(self.gens_var):\n",
    "                theta = self.weights[i][j]\n",
    "                U = torch.matrix_exp(1j * theta * G)\n",
    "                state = torch.matmul(U, state)\n",
    "\n",
    "        # density matrix\n",
    "        rho = state @ state.conj().transpose(-2, -1)\n",
    "        rho = (rho + rho.conj().transpose(-2, -1)) / 2  # ensure Hermitian\n",
    "\n",
    "        # fidelity\n",
    "        fidelities = []\n",
    "        for op in self.label_ops:\n",
    "            op_batch = op.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            product = rho @ op_batch\n",
    "            eigvals, _ = torch.linalg.eig(product)\n",
    "            eigvals_real = torch.clamp(eigvals.real, min=1e-10)\n",
    "            fidelities.append(torch.sum(torch.sqrt(eigvals_real), dim=1) ** 2)\n",
    "\n",
    "        fstack = torch.stack(fidelities, dim=1)\n",
    "        return fstack / fstack.sum(dim=1, keepdim=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
