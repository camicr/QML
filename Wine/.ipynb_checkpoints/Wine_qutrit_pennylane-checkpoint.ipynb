{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c24993f-93d1-4a67-956a-71b05ff53548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jax==0.4.23 in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (0.4.23)\n",
      "Requirement already satisfied: jaxlib==0.4.23 in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (0.4.23)\n",
      "Requirement already satisfied: optax==0.1.7 in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (0.1.7)\n",
      "Requirement already satisfied: pennylane==0.36.0 in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (0.36.0)\n",
      "Collecting scipy==1.10.1\n",
      "  Using cached scipy-1.10.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (53 kB)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from jax==0.4.23) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.22 in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from jax==0.4.23) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from jax==0.4.23) (3.4.0)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from optax==0.1.7) (2.1.0)\n",
      "Requirement already satisfied: chex>=0.1.5 in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from optax==0.1.7) (0.1.86)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from pennylane==0.36.0) (3.3)\n",
      "Requirement already satisfied: rustworkx in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from pennylane==0.36.0) (0.15.1)\n",
      "Requirement already satisfied: autograd in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from pennylane==0.36.0) (1.7.0)\n",
      "Requirement already satisfied: toml in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from pennylane==0.36.0) (0.10.2)\n",
      "Requirement already satisfied: appdirs in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from pennylane==0.36.0) (1.4.4)\n",
      "Requirement already satisfied: semantic-version>=2.7 in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from pennylane==0.36.0) (2.10.0)\n",
      "Requirement already satisfied: autoray>=0.6.1 in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from pennylane==0.36.0) (0.7.0)\n",
      "Requirement already satisfied: cachetools in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from pennylane==0.36.0) (5.3.3)\n",
      "Requirement already satisfied: pennylane-lightning>=0.36 in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from pennylane==0.36.0) (0.37.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from pennylane==0.36.0) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from pennylane==0.36.0) (4.11.0)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from chex>=0.1.5->optax==0.1.7) (0.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from requests->pennylane==0.36.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from requests->pennylane==0.36.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from requests->pennylane==0.36.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/QML-Qudits/lib/python3.10/site-packages (from requests->pennylane==0.36.0) (2024.8.30)\n",
      "Using cached scipy-1.10.1-cp310-cp310-macosx_12_0_arm64.whl (28.8 MB)\n",
      "Installing collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.3\n",
      "    Uninstalling scipy-1.15.3:\n",
      "      Successfully uninstalled scipy-1.15.3\n",
      "Successfully installed scipy-1.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jax==0.4.23 jaxlib==0.4.23 optax==0.1.7 pennylane==0.36.0 scipy==1.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64b2bd56-77aa-4ed7-a05c-2bf359152bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, cohen_kappa_score, matthews_corrcoef\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.utils import shuffle\n",
    "from copy import deepcopy\n",
    "import pennylane as qml\n",
    "import pennylane as qml\n",
    "from pennylane import math as qmath \n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)  # Habilita float64/complex128\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy.linalg as jsp\n",
    "import pennylane as qml\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7152ea2a-84e7-46b0-b606-126f10c956d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------- Gell-Mann apiladas (índice 1..8) --------\n",
    "def _gellmann_stack(dtype=jnp.complex64):\n",
    "    l0 = jnp.zeros((3,3), dtype=dtype)  # placeholder en 0\n",
    "    l1 = jnp.array([[0,1,0],[1,0,0],[0,0,0]], dtype=dtype)\n",
    "    l2 = jnp.array([[0,-1j,0],[1j,0,0],[0,0,0]], dtype=dtype)\n",
    "    l3 = jnp.array([[1,0,0],[0,-1,0],[0,0,0]], dtype=dtype)\n",
    "    l4 = jnp.array([[0,0,1],[0,0,0],[1,0,0]], dtype=dtype)\n",
    "    l5 = jnp.array([[0,0,-1j],[0,0,0],[1j,0,0]], dtype=dtype)\n",
    "    l6 = jnp.array([[0,0,0],[0,0,1],[0,1,0]], dtype=dtype)\n",
    "    l7 = jnp.array([[0,0,0],[0,0,-1j],[0,1j,0]], dtype=dtype)\n",
    "    l8 = (1/jnp.sqrt(3)) * jnp.array([[1,0,0],[0,1,0],[0,0,-2]], dtype=dtype)\n",
    "    return jnp.stack([l0,l1,l2,l3,l4,l5,l6,l7,l8], axis=0)\n",
    "\n",
    "LMATS = _gellmann_stack()  # (9,3,3) complejo\n",
    "\n",
    "# -------- Secuencia temporal (primero actúa λ8) --------\n",
    "TEMP_SEQ = (8, 3, 2, 3, 5, 3, 2, 3)\n",
    "\n",
    "def su3_unitary(alphas: jnp.ndarray, seq: tuple = TEMP_SEQ) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Construye U tal que al aplicar al estado se cumple:\n",
    "        U|ψ> = F_3 F_2 F_3 F_5 F_3 F_2 F_3 F_8 |ψ>\n",
    "    cuando len(alphas)=8 y seq = TEMP_SEQ; para n<8, usa el prefijo.\n",
    "    Convención: F_k(a) = exp(i * a * λ_k).  Acumulamos U = F @ U.\n",
    "    \"\"\"\n",
    "    n = alphas.shape[0]\n",
    "    ks = jnp.array(seq[:n], dtype=jnp.int32)\n",
    "\n",
    "    def body(U, ak):\n",
    "        a, k = ak\n",
    "        lam = LMATS[k]  # (3,3)\n",
    "        F = jsp.expm(1j * a.astype(jnp.complex64) * lam)\n",
    "        return F @ U, None  # pre-multiplico para respetar orden temporal\n",
    "\n",
    "    U0 = jnp.eye(3, dtype=jnp.complex64)\n",
    "    (U_final, _) = jax.lax.scan(body, U0, (alphas, ks))\n",
    "    return U_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7d0c259-6142-44e6-9ab1-c882581184a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- QNode (JAX) --------------------\n",
    "dev = qml.device(\"default.qutrit\", wires=1)\n",
    "\n",
    "@qml.qnode(dev, interface=\"jax\", diff_method=\"backprop\")\n",
    "def qcircuit(params, x):\n",
    "    for p in params:\n",
    "        \n",
    "        # encoding layer\n",
    "        qml.QutritUnitary(su3_unitary(x), wires=0)\n",
    "        # variational layer\n",
    "        qml.QutritUnitary(su3_unitary(p), wires=0)\n",
    "        \n",
    "    return qml.state()\n",
    "\n",
    "v_qcircuit = jax.vmap(qcircuit, in_axes=(None, 0), out_axes=0)\n",
    "\n",
    "# ---------- quantum labels (|0>, |1> as density matrices) ----------\n",
    "def create_dm_labels():\n",
    "    zero = jnp.array([1.0, 0.0, 0.0], dtype=jnp.float32)\n",
    "    one  = jnp.array([0.0, 1.0, 0.0], dtype=jnp.float32)\n",
    "    two  = jnp.array([0.0, 0.0, 1.0], dtype=jnp.float32)\n",
    "    return jnp.stack([zero, one, two]) \n",
    "\n",
    "dm_labels = create_dm_labels()\n",
    "\n",
    "# --------------------------- cost function --------------------------\n",
    "# def cost(params, X, y, dm_labels):\n",
    "#     # y debe ser int para indexar dm_labels[y]\n",
    "#     y = y.astype(jnp.int32)\n",
    "#     dm_y = dm_labels[y]                    # (batch, 2, 2)\n",
    "#     f = v_qcircuit(params, X, dm_y)    # (batch,)\n",
    "#     return jnp.mean((1.0 - f) ** 2)\n",
    "def cost(params, X, y, dm_labels):\n",
    "    states = v_qcircuit(params, X)        # (batch, 3)\n",
    "    targets = dm_labels[y]                # (batch, 3)\n",
    "\n",
    "    # fidelity = jnp.abs(jnp.vdot(states, targets)) ** 2\n",
    "    fidelity = jnp.sum(jnp.conj(states) * targets, axis=1)\n",
    "    fidelity = jnp.abs(fidelity) ** 2\n",
    "    return jnp.mean((1 - fidelity) ** 2)\n",
    "\n",
    "# ------------------------------ metrics ------------------------------\n",
    "# def predict(params, X, dm_labels):\n",
    "#     def fidelities_for_sample(xi):\n",
    "#         # evalúa fidelidad contra cada label_dm\n",
    "#         return jnp.array([qcircuit(params, xi, dm) for dm in dm_labels])\n",
    "#     fidelities = jax.vmap(fidelities_for_sample)(X)   # (batch, n_labels)\n",
    "#     return jnp.argmax(fidelities, axis=1)\n",
    "def predict(params, X, state_labels):\n",
    "    \"\"\"\n",
    "    Retorna la clase predicha para cada muestra de X,\n",
    "    usando fidelidad con los vectores `state_labels`.\n",
    "    \"\"\"\n",
    "    states = v_qcircuit(params, X)  # (batch, 3)\n",
    "\n",
    "    # Calculamos fidelidad con cada label\n",
    "    def fidelity_with_label(target_state):\n",
    "        # fidelity: batch de productos ⟨target|state⟩\n",
    "        inner = jnp.sum(jnp.conj(states) * target_state, axis=1)\n",
    "        return jnp.abs(inner) ** 2  # shape (batch,)\n",
    "\n",
    "    fidelities = jnp.stack([\n",
    "        fidelity_with_label(target_state)\n",
    "        for target_state in state_labels\n",
    "    ], axis=1)  # shape (batch, num_labels)\n",
    "\n",
    "    return jnp.argmax(fidelities, axis=1)  # clase con mayor fidelidad\n",
    "\n",
    "@jax.jit\n",
    "def accuracy(y_true, y_pred):\n",
    "    return jnp.mean((y_true.astype(jnp.int32) == y_pred.astype(jnp.int32)).astype(jnp.float32))\n",
    "\n",
    "def evaluate_metrics(X, y, params, dm_labels):\n",
    "    \"\"\"\n",
    "    Evalúa métricas de clasificación para el clasificador cuántico.\n",
    "\n",
    "    Devuelve:\n",
    "      - accuracy (%)\n",
    "      - f1 macro (%)\n",
    "      - precision macro (%)\n",
    "      - recall macro (%)\n",
    "      - cohen's kappa\n",
    "      - matthews corrcoef (MCC)\n",
    "      - y_true (numpy array)\n",
    "      - y_pred (numpy array)\n",
    "    \"\"\"\n",
    "    # y puede venir como DeviceArray -> pasamos a numpy\n",
    "    y_true = np.asarray(y)\n",
    "    # predict(...) ya vectoriza internamente con vmap\n",
    "    y_pred = np.asarray(predict(params, X, dm_labels))\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred) * 100.0\n",
    "    f1  = f1_score(y_true, y_pred, average=\"macro\", zero_division=0) * 100.0\n",
    "    prec = precision_score(y_true, y_pred, average=\"macro\", zero_division=0) * 100.0\n",
    "    rec  = recall_score(y_true, y_pred, average=\"macro\", zero_division=0) * 100.0\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    mcc   = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    return acc, f1, prec, rec, kappa, mcc, y_true, y_pred\n",
    "\n",
    "# --------------------------- training step ---------------------------\n",
    "@jax.jit\n",
    "def train_step(params, opt_state, X_batch, y_batch, dm_labels):\n",
    "    y_batch = y_batch.astype(jnp.int32)\n",
    "\n",
    "    loss_fn = lambda prms: cost(prms, X_batch, y_batch, dm_labels)\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state, loss\n",
    "\n",
    "# ------------------------ iterate_minibatches ------------------------\n",
    "def iterate_minibatches(X, y, batch_size):\n",
    "  n = X.shape[0]\n",
    "  for i in range(0, n, batch_size):\n",
    "    yield X[i:i+batch_size], y[i:i+batch_size]\n",
    "\n",
    "# ----------------- auxiliary function to plot data  -----------------\n",
    "def plot_data(X, labels, ax, title=\"\"):\n",
    "    X_np = np.array(X)\n",
    "    y_np = np.array(labels)\n",
    "    ax.scatter(X_np[y_np==0, 0], X_np[y_np==0, 1], s=10, alpha=0.7, label=\"Clase 0\")\n",
    "    ax.scatter(X_np[y_np==1, 0], X_np[y_np==1, 1], s=10, alpha=0.7, label=\"Clase 1\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"x1\")\n",
    "    ax.set_ylabel(\"x2\")\n",
    "    ax.legend(loc=\"best\", fontsize=8)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ffe1975-b86d-415b-b50e-d36fa15f3218",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "      ==================\n",
    "            data\n",
    "      ==================\n",
    "'''\n",
    "\n",
    "# create MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "SEED = 1337\n",
    "np.random.seed(SEED) # numpy\n",
    "key = jax.random.PRNGKey(SEED) # jax\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "def load_dataset(n_components):\n",
    "\n",
    "    # dataset\n",
    "    wine = load_wine()\n",
    "\n",
    "    # features\n",
    "    X_data = wine.data\n",
    "    X_data = scaler.fit_transform(X_data)\n",
    "\n",
    "    # labels\n",
    "    y_data = wine.target\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    x_data = pca.fit_transform(X_data)\n",
    "\n",
    "    x_data, y_data = shuffle(x_data, y_data, random_state=42)\n",
    "\n",
    "    return x_data, y_data, pca.explained_variance_ratio_.sum()\n",
    "\n",
    "\n",
    "# stratified split with seed (80% train / 20% test)\n",
    "X_np, y_np, _ = load_dataset(8)\n",
    "\n",
    "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
    "    X_np, y_np,\n",
    "    train_size=0.8,\n",
    "    test_size=0.2,\n",
    "    stratify=y_np,\n",
    "    shuffle=True,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "# numpy data to jax data\n",
    "X_train = jnp.array(X_train_np, dtype=jnp.float32)\n",
    "X_test  = jnp.array(X_test_np,  dtype=jnp.float32)\n",
    "y_train = jnp.array(y_train_np, dtype=jnp.int32)\n",
    "y_test  = jnp.array(y_test_np,  dtype=jnp.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80c5b317-a4df-4f2f-8d3b-eff617a77ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss: 0.5318 | Train Acc: 0.359 | Test Acc: 0.444\n",
      "Epoch 02 | Loss: 0.4339 | Train Acc: 0.472 | Test Acc: 0.583\n",
      "Epoch 03 | Loss: 0.3497 | Train Acc: 0.599 | Test Acc: 0.639\n",
      "Epoch 04 | Loss: 0.2879 | Train Acc: 0.655 | Test Acc: 0.639\n",
      "Epoch 05 | Loss: 0.2464 | Train Acc: 0.697 | Test Acc: 0.694\n",
      "Epoch 06 | Loss: 0.2205 | Train Acc: 0.704 | Test Acc: 0.722\n",
      "Epoch 07 | Loss: 0.2061 | Train Acc: 0.732 | Test Acc: 0.694\n",
      "Epoch 08 | Loss: 0.1992 | Train Acc: 0.739 | Test Acc: 0.722\n",
      "Epoch 09 | Loss: 0.1961 | Train Acc: 0.761 | Test Acc: 0.750\n",
      "Epoch 10 | Loss: 0.1945 | Train Acc: 0.782 | Test Acc: 0.750\n",
      "Epoch 11 | Loss: 0.1928 | Train Acc: 0.789 | Test Acc: 0.778\n",
      "Epoch 12 | Loss: 0.1902 | Train Acc: 0.789 | Test Acc: 0.778\n",
      "Epoch 13 | Loss: 0.1864 | Train Acc: 0.803 | Test Acc: 0.778\n",
      "Epoch 14 | Loss: 0.1818 | Train Acc: 0.796 | Test Acc: 0.778\n",
      "Epoch 15 | Loss: 0.1771 | Train Acc: 0.803 | Test Acc: 0.806\n",
      "Epoch 16 | Loss: 0.1727 | Train Acc: 0.810 | Test Acc: 0.806\n",
      "Epoch 17 | Loss: 0.1686 | Train Acc: 0.824 | Test Acc: 0.806\n",
      "Epoch 18 | Loss: 0.1649 | Train Acc: 0.838 | Test Acc: 0.806\n",
      "Epoch 19 | Loss: 0.1614 | Train Acc: 0.852 | Test Acc: 0.806\n",
      "Epoch 20 | Loss: 0.1582 | Train Acc: 0.859 | Test Acc: 0.806\n",
      "Epoch 21 | Loss: 0.1553 | Train Acc: 0.859 | Test Acc: 0.833\n",
      "Epoch 22 | Loss: 0.1527 | Train Acc: 0.852 | Test Acc: 0.833\n",
      "Epoch 23 | Loss: 0.1505 | Train Acc: 0.852 | Test Acc: 0.861\n",
      "Epoch 24 | Loss: 0.1485 | Train Acc: 0.852 | Test Acc: 0.861\n",
      "Epoch 25 | Loss: 0.1466 | Train Acc: 0.859 | Test Acc: 0.889\n",
      "Epoch 26 | Loss: 0.1449 | Train Acc: 0.859 | Test Acc: 0.889\n",
      "Epoch 27 | Loss: 0.1433 | Train Acc: 0.859 | Test Acc: 0.889\n",
      "Epoch 28 | Loss: 0.1418 | Train Acc: 0.859 | Test Acc: 0.889\n",
      "Epoch 29 | Loss: 0.1404 | Train Acc: 0.859 | Test Acc: 0.889\n",
      "Epoch 30 | Loss: 0.1391 | Train Acc: 0.866 | Test Acc: 0.889\n",
      "Epoch 31 | Loss: 0.1380 | Train Acc: 0.866 | Test Acc: 0.889\n",
      "Epoch 32 | Loss: 0.1371 | Train Acc: 0.866 | Test Acc: 0.889\n",
      "Epoch 33 | Loss: 0.1362 | Train Acc: 0.866 | Test Acc: 0.889\n",
      "Epoch 34 | Loss: 0.1354 | Train Acc: 0.866 | Test Acc: 0.889\n",
      "Epoch 35 | Loss: 0.1347 | Train Acc: 0.866 | Test Acc: 0.889\n",
      "Epoch 36 | Loss: 0.1340 | Train Acc: 0.873 | Test Acc: 0.889\n",
      "Epoch 37 | Loss: 0.1333 | Train Acc: 0.873 | Test Acc: 0.889\n",
      "Epoch 38 | Loss: 0.1326 | Train Acc: 0.866 | Test Acc: 0.889\n",
      "Epoch 39 | Loss: 0.1319 | Train Acc: 0.866 | Test Acc: 0.889\n",
      "Epoch 40 | Loss: 0.1311 | Train Acc: 0.866 | Test Acc: 0.889\n",
      "Epoch 41 | Loss: 0.1302 | Train Acc: 0.866 | Test Acc: 0.889\n",
      "Epoch 42 | Loss: 0.1293 | Train Acc: 0.873 | Test Acc: 0.889\n",
      "Epoch 43 | Loss: 0.1282 | Train Acc: 0.873 | Test Acc: 0.889\n",
      "Epoch 44 | Loss: 0.1271 | Train Acc: 0.873 | Test Acc: 0.889\n",
      "Epoch 45 | Loss: 0.1258 | Train Acc: 0.873 | Test Acc: 0.889\n",
      "Epoch 46 | Loss: 0.1245 | Train Acc: 0.873 | Test Acc: 0.889\n",
      "Epoch 47 | Loss: 0.1231 | Train Acc: 0.873 | Test Acc: 0.889\n",
      "Epoch 48 | Loss: 0.1215 | Train Acc: 0.873 | Test Acc: 0.889\n",
      "Epoch 49 | Loss: 0.1200 | Train Acc: 0.873 | Test Acc: 0.889\n",
      "Epoch 50 | Loss: 0.1184 | Train Acc: 0.873 | Test Acc: 0.889\n",
      "Epoch 51 | Loss: 0.1168 | Train Acc: 0.873 | Test Acc: 0.889\n",
      "Epoch 52 | Loss: 0.1152 | Train Acc: 0.887 | Test Acc: 0.889\n",
      "Epoch 53 | Loss: 0.1136 | Train Acc: 0.887 | Test Acc: 0.889\n",
      "Epoch 54 | Loss: 0.1121 | Train Acc: 0.887 | Test Acc: 0.889\n",
      "Epoch 55 | Loss: 0.1106 | Train Acc: 0.894 | Test Acc: 0.889\n",
      "Epoch 56 | Loss: 0.1092 | Train Acc: 0.894 | Test Acc: 0.889\n",
      "Epoch 57 | Loss: 0.1078 | Train Acc: 0.894 | Test Acc: 0.889\n",
      "Epoch 58 | Loss: 0.1065 | Train Acc: 0.908 | Test Acc: 0.889\n",
      "Epoch 59 | Loss: 0.1052 | Train Acc: 0.908 | Test Acc: 0.889\n",
      "Epoch 60 | Loss: 0.1040 | Train Acc: 0.908 | Test Acc: 0.889\n",
      "Epoch 61 | Loss: 0.1028 | Train Acc: 0.908 | Test Acc: 0.889\n",
      "Epoch 62 | Loss: 0.1017 | Train Acc: 0.908 | Test Acc: 0.889\n",
      "Epoch 63 | Loss: 0.1006 | Train Acc: 0.908 | Test Acc: 0.889\n",
      "Epoch 64 | Loss: 0.0995 | Train Acc: 0.901 | Test Acc: 0.889\n",
      "Epoch 65 | Loss: 0.0985 | Train Acc: 0.901 | Test Acc: 0.917\n",
      "Epoch 66 | Loss: 0.0975 | Train Acc: 0.901 | Test Acc: 0.917\n",
      "Epoch 67 | Loss: 0.0966 | Train Acc: 0.901 | Test Acc: 0.917\n",
      "Epoch 68 | Loss: 0.0957 | Train Acc: 0.901 | Test Acc: 0.917\n",
      "Epoch 69 | Loss: 0.0949 | Train Acc: 0.901 | Test Acc: 0.917\n",
      "Epoch 70 | Loss: 0.0940 | Train Acc: 0.901 | Test Acc: 0.944\n",
      "Epoch 71 | Loss: 0.0933 | Train Acc: 0.901 | Test Acc: 0.944\n",
      "Epoch 72 | Loss: 0.0925 | Train Acc: 0.901 | Test Acc: 0.944\n",
      "Epoch 73 | Loss: 0.0918 | Train Acc: 0.901 | Test Acc: 0.944\n",
      "Epoch 74 | Loss: 0.0911 | Train Acc: 0.901 | Test Acc: 0.972\n",
      "Epoch 75 | Loss: 0.0904 | Train Acc: 0.894 | Test Acc: 0.972\n",
      "Epoch 76 | Loss: 0.0898 | Train Acc: 0.894 | Test Acc: 0.972\n",
      "Epoch 77 | Loss: 0.0892 | Train Acc: 0.894 | Test Acc: 0.972\n",
      "Epoch 78 | Loss: 0.0886 | Train Acc: 0.901 | Test Acc: 0.972\n",
      "Epoch 79 | Loss: 0.0880 | Train Acc: 0.901 | Test Acc: 0.972\n",
      "Epoch 80 | Loss: 0.0875 | Train Acc: 0.908 | Test Acc: 0.972\n",
      "Epoch 81 | Loss: 0.0870 | Train Acc: 0.908 | Test Acc: 0.972\n",
      "Epoch 82 | Loss: 0.0865 | Train Acc: 0.908 | Test Acc: 0.972\n",
      "Epoch 83 | Loss: 0.0860 | Train Acc: 0.908 | Test Acc: 0.972\n",
      "Epoch 84 | Loss: 0.0856 | Train Acc: 0.908 | Test Acc: 0.972\n",
      "Epoch 85 | Loss: 0.0851 | Train Acc: 0.908 | Test Acc: 0.972\n",
      "Epoch 86 | Loss: 0.0847 | Train Acc: 0.908 | Test Acc: 0.972\n",
      "Epoch 87 | Loss: 0.0843 | Train Acc: 0.908 | Test Acc: 0.972\n",
      "Epoch 88 | Loss: 0.0839 | Train Acc: 0.908 | Test Acc: 0.972\n",
      "Epoch 89 | Loss: 0.0835 | Train Acc: 0.908 | Test Acc: 0.972\n",
      "Epoch 90 | Loss: 0.0832 | Train Acc: 0.915 | Test Acc: 0.972\n",
      "Epoch 91 | Loss: 0.0828 | Train Acc: 0.915 | Test Acc: 0.972\n",
      "Epoch 92 | Loss: 0.0825 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 93 | Loss: 0.0822 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 94 | Loss: 0.0819 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 95 | Loss: 0.0815 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 96 | Loss: 0.0812 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 97 | Loss: 0.0810 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 98 | Loss: 0.0807 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 99 | Loss: 0.0804 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 100 | Loss: 0.0801 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 101 | Loss: 0.0799 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 102 | Loss: 0.0796 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 103 | Loss: 0.0794 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 104 | Loss: 0.0792 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 105 | Loss: 0.0789 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 106 | Loss: 0.0787 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 107 | Loss: 0.0785 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 108 | Loss: 0.0783 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 109 | Loss: 0.0781 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 110 | Loss: 0.0779 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 111 | Loss: 0.0777 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 112 | Loss: 0.0775 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 113 | Loss: 0.0773 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 114 | Loss: 0.0771 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 115 | Loss: 0.0769 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 116 | Loss: 0.0768 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 117 | Loss: 0.0766 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 118 | Loss: 0.0764 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 119 | Loss: 0.0763 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 120 | Loss: 0.0761 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 121 | Loss: 0.0760 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 122 | Loss: 0.0758 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 123 | Loss: 0.0757 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 124 | Loss: 0.0756 | Train Acc: 0.923 | Test Acc: 0.917\n",
      "Epoch 125 | Loss: 0.0754 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 126 | Loss: 0.0753 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 127 | Loss: 0.0752 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 128 | Loss: 0.0750 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 129 | Loss: 0.0749 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 130 | Loss: 0.0748 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 131 | Loss: 0.0747 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 132 | Loss: 0.0746 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 133 | Loss: 0.0744 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 134 | Loss: 0.0743 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 135 | Loss: 0.0742 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 136 | Loss: 0.0741 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 137 | Loss: 0.0740 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 138 | Loss: 0.0739 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 139 | Loss: 0.0738 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 140 | Loss: 0.0737 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 141 | Loss: 0.0736 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 142 | Loss: 0.0735 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 143 | Loss: 0.0734 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 144 | Loss: 0.0733 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 145 | Loss: 0.0732 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 146 | Loss: 0.0731 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 147 | Loss: 0.0730 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 148 | Loss: 0.0730 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 149 | Loss: 0.0729 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 150 | Loss: 0.0728 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 151 | Loss: 0.0727 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 152 | Loss: 0.0726 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 153 | Loss: 0.0725 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 154 | Loss: 0.0724 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 155 | Loss: 0.0724 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 156 | Loss: 0.0723 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 157 | Loss: 0.0722 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 158 | Loss: 0.0721 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 159 | Loss: 0.0720 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 160 | Loss: 0.0719 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 161 | Loss: 0.0719 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 162 | Loss: 0.0718 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 163 | Loss: 0.0717 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 164 | Loss: 0.0716 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 165 | Loss: 0.0716 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 166 | Loss: 0.0715 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 167 | Loss: 0.0714 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 168 | Loss: 0.0713 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 169 | Loss: 0.0712 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 170 | Loss: 0.0712 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 171 | Loss: 0.0711 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 172 | Loss: 0.0710 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 173 | Loss: 0.0709 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 174 | Loss: 0.0709 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 175 | Loss: 0.0708 | Train Acc: 0.915 | Test Acc: 0.944\n",
      "Epoch 176 | Loss: 0.0707 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 177 | Loss: 0.0706 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 178 | Loss: 0.0706 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 179 | Loss: 0.0705 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 180 | Loss: 0.0704 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 181 | Loss: 0.0703 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 182 | Loss: 0.0703 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 183 | Loss: 0.0702 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 184 | Loss: 0.0701 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 185 | Loss: 0.0700 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 186 | Loss: 0.0700 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 187 | Loss: 0.0699 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 188 | Loss: 0.0698 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 189 | Loss: 0.0697 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 190 | Loss: 0.0697 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 191 | Loss: 0.0696 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 192 | Loss: 0.0695 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 193 | Loss: 0.0694 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 194 | Loss: 0.0694 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 195 | Loss: 0.0693 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 196 | Loss: 0.0692 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 197 | Loss: 0.0691 | Train Acc: 0.923 | Test Acc: 0.944\n",
      "Epoch 198 | Loss: 0.0691 | Train Acc: 0.930 | Test Acc: 0.944\n",
      "Epoch 199 | Loss: 0.0690 | Train Acc: 0.930 | Test Acc: 0.944\n",
      "Epoch 200 | Loss: 0.0689 | Train Acc: 0.930 | Test Acc: 0.944\n",
      "Total time: 870.77sTrain Loss: 0.075618 | Test Loss: 0.086540 | Acc: 94.44% | F1: 94.29% | Prec: 94.29% | Rec: 94.29% | κ: 0.916 | MCC: 0.916\n"
     ]
    }
   ],
   "source": [
    "''' -----------------------------------\n",
    "             1 qutrit training\n",
    "    -----------------------------------\n",
    "'''\n",
    "\n",
    "# init\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_LAYERS    = 6\n",
    "INIT_PARAMS   = jax.random.uniform(key, shape=(NUM_LAYERS, 8), minval=-1.0, maxval=1.0)\n",
    "optimizer     = optax.adam(LEARNING_RATE)\n",
    "opt_state     = optimizer.init(INIT_PARAMS)\n",
    "EPOCHS        = 200\n",
    "BATCH_SIZE    = 32\n",
    "\n",
    "# initiate timer\n",
    "start_time = time.time()\n",
    "\n",
    "# start training\n",
    "params = INIT_PARAMS\n",
    "for epoch in range(EPOCHS):\n",
    "    for Xb, yb in iterate_minibatches(X_train, y_train, BATCH_SIZE):\n",
    "        params, opt_state, loss = train_step(params, opt_state, Xb, yb, dm_labels)\n",
    "\n",
    "    y_pred_train = predict(params, X_train, dm_labels)\n",
    "    y_pred_test  = predict(params, X_test,  dm_labels)\n",
    "    acc_train = accuracy(y_train, y_pred_train)\n",
    "    acc_test = accuracy(y_test,  y_pred_test)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | Loss: {float(loss):.4f} | Train Acc: {float(acc_train):.3f} | Test Acc: {float(acc_test):.3f}\")\n",
    "\n",
    "# total elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# metrics\n",
    "final_loss_train = float(cost(params, X_train, y_train, dm_labels))\n",
    "acc_train, _, _, _, _, _, _, _ = evaluate_metrics(X_train, y_train, params, dm_labels)\n",
    "\n",
    "final_loss_test = float(cost(params, X_test, y_test, dm_labels))\n",
    "acc_test, f1, prec, rec, kappa, mcc, y_true, y_pred = evaluate_metrics(X_test, y_test, params, dm_labels)\n",
    "\n",
    "# print metrics\n",
    "print(\n",
    "    f\"Total time: {elapsed_time:.2f}s\"\n",
    "    f\"Train Loss: {final_loss_train:.6f} | \"\n",
    "    f\"Test Loss: {final_loss_test:.6f} | \"\n",
    "    f\"Acc: {acc_test:.2f}% | F1: {f1:.2f}% | \"\n",
    "    f\"Prec: {prec:.2f}% | Rec: {rec:.2f}% | \"\n",
    "    f\"κ: {kappa:.3f} | MCC: {mcc:.3f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6448acf-b9cb-4774-ae93-6eefe8b85ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
